

{{TOC}}

## L1

Repeats held on 2 Mon, 1 Tue and 1 Wed if main one is full
ECHO360 on WATTLE for recorded audio
steven.roberts@anu.edu.au, 4.43 CBE
Tutorials on ETA, begin in week 2
Read through tutorial sheet and think about questions **before** class
MyStatLab not compulsory
**RECOMMENDED**: First four pages of Intro to R (on WATTLE)
Referencing assignment -> also need to fill out assignment cover sheet
**ASSESSMENT**
Two assignments due March 23^rd and May 4^th - worth 15% each. *Late not acccepted*
3 hour exam worth 65%. Allowed:
- [ ] non-prg calc
memes
- [ ] dictionary
- [ ] one A4 double-sided sheet
Reference and writing exercise - 5%, due March 18^th

Stats: collecting, organising and interpreting data
*(Check presentation for links to view)*

Actually come, be prepared ( read notes, attempt tutorial questions)





## L2

Maths revision presentations available on WATTLE (probs not necessary, but check them anyway)
#### **DATA COLLECTION**
Stats: a way to get information from data
**Statistical inference**: the problem of determining the behaviour of a large population by studying a small sample from that population, i.e. making an estimate based on a sample
**Population**: a collection of the whole of something
**Sample**: A set of individuals drawn from a population
**Parameters**: true values for things like the centre and spread of the population
**Statistics**: these are values that estimate the parameters, need to use *inference* to do this estimation.
**Census**: a study of all elements of a population
**ERRORS**:
- **Sampling error**: the differences that exist between a population and a sample asa result of the sample selection. Will be reduced by a larger sample
- **NonSampling error**: larger samples won’t reduce nonsampling error so more serious - includes coverage error, nonresponse error, measurement error.
- **Coverage error**:
	- results from selection bias
	- reduced by having an adequate sampling frame (frame = up-to-date list of all items from which you will select the sample)
- **Nonresponse error**:
	- Research has shown that pepople in upper and lower economic classes respond less frequently to surveys than those in middle class
	- Results in nonresponse bias - can’t assume that people who do respond to surveys are the same as those who don’t
	- Need to follow up on non-responses
- **Measurument Errors**
	- **Ambiguous wording**: want questions to be clear
	- **Hawthorne effect**:
		- Respondent feels obligated to please the interviewer
		- Need to train interviewers well to minimise this effect
		- Want questions and interviewer to be neutralm not leading
	- **Respondent error**: Minimised by screening responses and following up unusual answers, and establishing a program of re-contacting people to follow up answers

Read about the Literary Digest poll

## L3

Assignment workshop info thing info available on WATTLE
**Ethical Issues**
- Coverage error becomes an ethical issue if particular groups/individuals are purposely exluded
- Nonresponse error becomes an ethical issue if survey designed such that some groups are less likely to respond
- Sampling error becomes an ethical issue if findings purposely presented without reference to survey size and margin of error
- Measurement error becomes an ethical issue if leading questions used, intentional Hawthorne effect, respondent provides false answers

**Sample frame**: list of all items that make up the population, choice of this is important. Sample is then chosen from the sample frame.
**Nonprobability or probability sampling**: *Nonprob* choose the items to be invleded withouth knowing their probabilities of selection, stat inference can’t be used. *Prob* select items based on bnown probabilities, **can** use stat inference.
Look at [socialresearhmethods.com](https://www.socialresearchmethods.com) sampling

**Convenience sampling**:using clients available to us as a sample.

**Judgmental sampling**: involves selection of experts in cmoe areas. However, experts are often inaccurate.

**Quota sampling**: select people nonrandomly accordding to some fixed quota. Two categories: proportional or non-proportional quota sampling. For example, if you know therer are 60% men and 40% women in the population, then out of 100 60 men and 40 women would be chosen. Non-proportional less restrictive, specify the minimum number of sampling units in each category.

**Snowball sampling**:begin by identifying someone who meets the selection criteria in your study, and then ask him/her to refer to others who they think also satisfy these selection criteria. Disadvantage that you can’t assess the sampling quality.

**Simple random sampling**:
- Simplest probabilistic sampling, it is reasonable to generalize the results from the sample to the population.
- Aims to select n units out of N (total units in population) such that each element of your sample has an equal chance of being selected
- Disadvantage that you not get a good representation of subgroups in a population due to chance.

**Systematic random sampling**:
- Number the units in the population from 1 to N
- Decide the n (sample size) we want or need
- k = N/n = interval size
- Randomly draw an integer between 1 and k
- E.g. if k = 4, sample units 4, 14, 24, 34, etc.

**Stratified random sampling**:
- Smetimes called porportional or quota sandom sampling, invlolving dividing our population into homogeneous subgroups and then taking a simple random sample in each subgroup.
- Sex, Religion, Race are categories for identifying subgroups.

**Cluster/Area random sampling**:
- Divide population into clusters (usually geographic boundaries)
- Randomly sample these clusters
- Measure **all** units within sampled clusters

## L4

Re-read rats in ny
Covering chapters 2 and 3, have a quick read of these

**DESCRIPTIVE STATISTICS**
Summarizing and presenting data so that features of data can be interpreted. Both *graphical* and *numerical*.
**Variables**: Feature of population which is of interest, e.g. height of female students.
**Data**: Actual observations of variables.
Two basic types of data - categorical or numerical
**Categorical Data**: nominal, ordinal, e.g. e.g. faculty of study, eye colour, job for nominal, and rank teaching as poor/fair/good/verygood, house numbers and swimming levels for ordinal.
**Numerical Data**: discrete (count) or continuous (also called interval), 
Other stuff, e.g. comments in interview/survey are *qualitative*

Tools for Categorical Data
- Graphs
-  Bar charts
-  Pie charts (**beware**)
-  **Get the rest of these from the lecture slides**

**Location of Percentiles**: L<sub>p</sub> = ((n+1)P)/100, where n is the number of data points, L<sub>p</sub> is the location of the p^th percentile, then can go 

## L5 
**Cumulative Relative Frequency Distribution**
- Relative Frequency Distribution Histogram - proportion in each class.
- Cumulative Relative Frequency Distribution - proportion up to and including that class.
- Ogive - graph of cumulative relative frequencies; also called empirical cumulative density function.

## L6

**Relative Frequency**: The frequency of a value in proportion to the total number of samples taken. E.g. If 10 were sampled, and 1 was a value with a frequency of one, then it’s relative frequency would be 0.1
**Cumulative Relative Frequency**: Add together relative frequencies up to that point.
For Boxplots, need MEDIAN and QUATILES to create. 
**Interquartile Range** = Q<sub>3</sub> - Q<sub>1</sub>
**Skewing**: Negative: ->, Positive: <-
Multimodality difficult to determine from boxplots.  
**Quartiles**: Divide the data into quartiles. Tertiles divide into thirds, Quintiles fifths, etc.
**Percentiles**: Divide the data into percents (hundredths).
L<sub>p</sub> = ((n+1)P)/100 gives you the number of the *observation*, the observation is the number/place of the value
- L<sub>p</sub> is the location of of the p^th percentile
- n = number of observations

**Arithmetic Mean**: average
**Sample average** is called “Xbar”, written as X with line across the top.

## L7 

R not part of examination
Mode not appropriate for continuous data
Mean usually best for measure of centre, but is sensitive to extreme values, so use the median in that case
If data set is:
- **Symmetric**: mean=median
- **Positive skew**: mean>median
- **Negative skew**: mean<median
**Covariance**: Measures the linear relationship between X and Y - sign indicates direction of slope, magnitude not really useful
**Coefficient of Correlation**: Also measures strength of linear relationship between X and Y, is bounded between -1 and +1
Really need to do this, because they’re talking about some shit to do with an assignment

## L8

Populations have parameters, from samples we calculate statistics.

Gapminder
Go over slides in own time to look at all the pretty graphs


## L9


## L10
Probability!

## L11

Assignment coming soon
S = sample space
Probability is  always between 0 and 1
Sum of all prob. is always 1
Complement of an event is the set of all outvomes not belonging to that event
**Complement Rule**: A + A^c = S
**Multiplication Rule**: P(A union B) = P(A|B)P(B) and P(B|A)P(A)
Read textbook ch. 4
Random variables in capitals, actual values in lower case.


## L12

**Random Variables**: If value of X depends on chance, then it is a *random variable*

## L13

Ask course dude about whether or not my r is acceptable for a screenshot

var(cbind(x,y)), want the ones that match, could just use cov(x,y)
**Grab the formulae for the Rules for Expectations form the lecture slides**
E(c) = c 
E(cX) = cE(X)
E(X-Y) = E(X)-E(Y)
E(X+Y) = E(X)+E(Y)
E(XY) = E(X)*E(Y) **only** if X and Y are independent

**Variance**: Measures spread/dispersion of distribution
σ<sup>2</sup> = E[(X-µ)<sup>2</sup>] 
Var(X) = E(X^2 )-µ^2
Var(X) = ∑[x<sub>i</sub><sup>2</sup> * p(x<sub>i</sub>)]- σ^2 
Var(X)=σ^2(X) = σ<sub>X</sub>^2
V(c)=0
V(cX)=c^2V(X)
V(X+c)=V(X)
V(X+Y)=V(X)+V(Y) (*if X and Y are independent*)
V(X-Y) = V(X)+V(Y) (*if X and Y are indepent*)


## L14

**Linear Combinations of Random Variables**
If Z=aX+bY, where a and b are constants, and X and Y are random variables, it can be shown that:
E(aX + bY) = aE(X) + bE(Y)
V(aX + bY) = a<sup>2<sup>V(X) + b<sup>2<sup>V(Y) + 2*av*cov(X,Y)
	= a<sup>2</sup>σ<sup>2</sup><sub>x</sub>+b<sup>2</sup>σ<sup>2</sup><sub>y</sub> + 2avpσ<sub>x</sub><sub>y</sub>

## L15
**Frequentist Probabilities**: Probs. associated with r.v.s are ofen associated with relative frequencies. i.e. Estimate probabilites from frequencies.

## L16

**Univariate**: Distribution of a single variable
**Bivariate**: Distribution of two variables together
So, if X and Y are discrete random variables, then we say p(x,y) = P(X=x and Y=y) is the joint probability that X=x and Y=y

**Marginal Probability Function**: In general, marginal distribution of X is given by p(x) = P(X=x) = ∑(all y) p(x,). E.g. P(X=1) = p(1,0) + p(1,1) + p(1,2)
**Independence**: If random variables X and Y are independent, then P(X=x and Y=y) = P(X=x).P(Y=y)
p(x,y) = p<sub>x</sub>(x).p<sub>x</sub>(y)
**Covariance**: If E(X)=µ<sub>x</sub> and E(Y)=µ<sub>y</sub>, then the covariance between X and Y is given by :
σ<sub>xy</sub> = cov(X,Y) = E[(X-µ<sub>x</sub>)(Y-µ<sub>y</sub>)]
**Correlation Coefficient**: See book

## L17
**Combinations**: See book
**Binomial Notation**: If X is a binomial random variable with n trials and p is the probability of success in each trial, then we write X~Bin(n,p)
P(X=k) when X~Bin(n,p) means the probability that we will have k successes and (n-k) failures; we know how many ways we arrange those successes and failures...
**Binomial Formula**: See book
**Mean and Variance of a Binomial Distribution**:
µ<sub>x</sub> = E(X) = np
σ<sup>2</sup><sub>X</sub> = np(1-p)

*Binomial tables in back of textbook are pretty useful*

## L18
**Normal Distribution**: X~N(µ,σ^2 )

## L19
**Uniform Distribution**: ƒ(x)=1/(b-a) 
**Normal Distribution**: See book for pdf. Bell shaped, symmetric about µ, reaches highest point at x=µ, tends to zero as x->±∞. 
- Area under curve = 1
- Different means shift curve up and down x axis
- Different variances cause the curve to become more peaked or squashed
- Notation: X~N(µ,σ<sup>2</sup>)
**Standard Normal Distribution**: Z~N(0,1)

## L?? (figure this out)

**Central Limit Theorem**: Xbar -> N(µ, σ^2/n), (Xbar - µ)/(σ/√n)

**Standard error of the mean**: σ/√n, just means spread

For these problems, remember to check if it's a normal distribution or not. If you don't do thatm you in trouble.

in R, use nfrow to get more than one graph on the same output buffer.

## L??+1

For central limit theorem, Xbar follows normal distribution with E(Xbar)=µ and var(Xbar)=σ^2/n

This happens regardless of the shape of the original population

**What size n?**
If the distribution of X is normal, then for all n the sample mean will follow a normal distribution
If the distribution of X is *very* not normal, then we will need a large n for us to see the normality of the distribution of the sample mean.
In all cases, as n gets larger, the distribution of the mean gets more normal

**Types of Estimators**
**Point Estimate**: a single value or point, i.e. sample mean = 4 is a point estimate of the population mean, µ
**Interval Estimate**: Draws inferences about a population by estimating a parameter using an interval (range). E.g. We are 95% confident that the unknown mean score lies between 56 and 78
Want estimators to be *unbiased*. An unbiased estimator is consistent if the difference between the estimator and the parameter gets smaller as the sample gets larger.
1.645 = 90%
1.96  = 95%
2.33  = 98%
2.575 = 99%

## L??+2

This shit needed for the assignment
Don't forget to write the formula for general confidence interval


## L??
alpha = significance = P(type 1 error)
beta  = 1 - power = P(type 2 error)
Power = P(reject H<sub>0</sub>, when it is false)

**Null Hypothesis**
- Always about a population value (greek letter)
- Always has an "="

**Alternative Hypothesis**
- Always about a population value (greek letter)
- Has one of <, > or =/=
- Looks like null, but "=" has been replaced


For this example:
H<sub>0</sub>: mu = 170
H<sub>A</sub>: mu > 170
Z should follow a standard normal distribution IF the true mu is equal to the one in our null hypothesis
If the NH is true, then value should look like a value from Z distribution
**Need to watch the lecture from before this one, which is lecture C from last week**

**One Tailed vs Two Tailed tests**

If the alt. hypothesisis < or >
- this is a one tailed test
- rejection region wil be in *either* upper *or* lower tail
- P-value is the probability of getting a more extreme result

If the alt. hypothesis is =/=
- this is a two tailed test
- rejection region needs to be split between both tails
- P-value will include an absolute value - i.e. will be the probability of getting further away from the hypothesised mean on either side

If alt. is <
- Left tailed
- Rejection region will be Z<-Z<sub>alpha</sub>
- P-value will be P(Z<T.S)

If alt. is >
- Right tailed
- RR will be Z>+Z<sub>alpha</sub>
- P value will be P(Z>T.S)

If alt. is =/=
- Two tailed
- RR wil be Z<-Z<sub>alpha/2</sub>, Z>+Z<sub>alpha/2</sub>
- P-V will be P(Z>|T.S|) + P(Z<-|T.S|)

Xbar - µ / (s/sqrt(n)) follows a t-distribution with n-1 *degrees of freedom*
t-distribution

## Lsomethingorother
**Confidence intervals for µ when sigma is unknown** 

Test statistic = (Xbar - µ)/(sample variance/sqrt(n))
General form of CI the same, (i think?), but with t and s substituted in

## Ltheoneaftertheonebefore


## Lafter that one

some formulae for something (you'll find out in the tutorial)
paired samples apparently much easier to deal with - calculate differences, then perform one-sample test on these differences, i.e. have to samples, for each observation calculate X|d| = X|1| - X|2|
then calculate Xbar|d| and s|d| (mean and standard deviation of differences)

Procedure:
1. Hypotheses
2. Test Statistic
3. Decision Rule
4. Conclusion

##LANOTHERONE

Allowed 2 single sided pages of notes, so get your formulae on there
Given tables, so don't need to write values
So just formulae because they will be helpful

