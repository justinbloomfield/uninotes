#+STARTUP: indent latexpreview
#+OPTIONS: toc:nil
* Notes
** Calculus
*** W8
**** Integration of Rational Functions, Section 9.5
     for shit in the form of $f(x)=\frac{g(x)}{h(x)}$
     1. If the numerator is a higher degree function than the denominator, must use long division
        - [ ] Add example to cheat sheet
     2. Use partial fractions to separate function
        - Factorise denominator first, to split
        - do the other stuff you need to do (shouldn't need an example for this)
     3. Integrate as normal
**** Improper Integrals, Section 9.6
     An integral is *improper* if it is unbounded at an end point, or if an endpoint is infinite.
     - [ ] Add example to cheat sheet (be sure to include the limits)
**** Taylor Polynomials, Sections 10.1-10.2
     *Lagrange's Remainder Theorem*
     $En(x)= \frac{f^{(n+1)}(x_{0})}{(n+1)!}(x-a)^{n+1}$
     Used for determining the error in the approximation

     *Taylor Polynomials*
     $\sum^{n}_{i=0}\frac{f^{(i)}(a)}{i!}(x-a)^{i}$

     - [ ] Defs example for this one
*** W9
**** Taylor Series, Sections 10.3-10.5
     The *Taylor Series* of $f$ about $a$ is the infinite sum:
     $\sum^{\infty}_{i=0}\frac{f^{(i)}(a)}{i!}(x-a)^{i}$ 
     
     I don't have a fucking clue how to do these, so do make sure to 
     - [ ] include an example
*** W10
**** Differential Equations, Section 11.1-11.5
     *Ordinary*: single variable
     An *ODE* is _separable_ if it can be written in the form:
     $f(y)\frac{dy}{dx}=g(x)$
     Solve by "multiplying by dx" and integrating:
     $\int{f(y)dy}=\int{g(x)dx}$
     
     *First Order Linear DEs*
     $y'+p(x)y=q(x)$
     if $q(x)=0$, this is separable, and solvable using:
     $y=e^{-P(x)}\int{e^{P(x)}q(x)dx}$, where $p'(x)=p(x)$
     
     - [ ] example of something similar the salt water one, or practise several of them
**** Functions of Several Variables, Section 12.1
     just graphs of shit, hopefully not important LUL
*** W11
**** Domains and Subsets of R^n, Section 12.2
     IDEK FUCK ME DADDY
**** Limits and Continuity, Section 13.1
**** Partial Derivatives, Section 13.1
     $\frac{\delta f}{\delta x}=f_{1}=D_{1}f$
     for $\frac{\delta f}{\delta x}$, differentiate for x, treat y as a constant, opposite for y
     for the more complicated shit, $\frac{\delta^{2} f}{\delta x \delta y} = \frac{\delta f}{\delta y}(\frac{\delta f}{\delta x})$ and such like
     Theorem of $\frac{\delta^{2} f}{\delta x \delta y}(a,b)=\frac{\delta^{2} f}{\delta y \delta x}(a,b)$. If this isn't working, check continuity of function
**** Linear Approximation, Section 13.2
     $f(x+ \Delta x, y+\Delta y) \approx f(x,y)+\frac{\delta f}{\delta x}(x,y)\Delta x + \frac{\delta f}{\delta y}(x,y)\Delta y$
     the /differential/ of f(x,y) at (a,b) is
     $df(a,b)=\frac{\delta f}{\delta x}(a,b)dx+\frac{\delta f}{\delta y}(a,b)dy$
     - [ ] example?
**** Differentiability, Section 13.3
     hand write this formula, fuck putting that in LaTeX
     f differentiable at (a,b) => f continuous at (a,b)
**** The Chain Rule, Section 13.4
     hand write this as well, it's gay to write but doesn't seem too hard to use, maybe an example but probs not as long as I've practiced some of them
*** W12
**** Gradients and Directional Derivatives, Section 13.5
$\frac{\delta f}{\delta x}$ measures RoC in x-direction
$\frac{\delta f}{\delta y}$ measures RoC in y-direction
The /gradient/ of f is:
$\nabla f = \frac{\delta f}{\delta x}\vec{i} + \frac{\delta f}{\delta y}\vec{j} (+\frac{\delta f}{\delta z}\vec{k})$
looooot of shit for this one, make sure to practise
**** Extrema and Optimisation, Section 13.6
Critical points:
- Stationary point :: where $\nabla f(a,b)=0$
- Singular point :: where $\nabla f(a,b)$ does not exist
- Boundary point :: where (a,b) is on the boundary of the domain of f
Local min/max can only occur at critical points
=there is a lot more than this, make sure to get the important parts=
**** Multivariable Integration, Sections 14.1-14.2
*** W13
**** More Multivariable Integration, Sections 14.3-14.5
** Algebra
*** W8
**** Complex Numbers, Lay App. B, Adams App. 1
(a+bi)(c+di)=(ac-bd)+(ad+bc)i
z*zbar=(a+bi)(a-bi)=a^2+b^2
$z^-1=\frac{1}{a^{2}+b^{2}}(a-bi)$
$|z|=\sqrt{a^{2}+b^{2}}$
Can write z in _polar form_:
$z=r(cos\theta+ isin\theta)$, for some 0\le\theta\le2\pi
*principle argument* of z is \theta, denoted by Arg(z)
- [ ] roots
- [ ] fundamental theorem of algebra
- [ ] matrix solve example
**** Invertible Matrices, Lay 2.2-2.3
An nxn matrix A is invertible if there is another nxn matrix B such that $AB=BA=In$
If A is invertible, then the inverse is unique (there's a proof for this)
/A is invertible if detA\ne0/
A square matrix is invertible if and only if its row equivalent to I
- [ ] example of inverting a 3x3 maybs, defs at least a 2x2, maybe a non-nxn one too, if we did those
*** W9
**** Determinants, Lay 3.1-3.2
- [ ] determinant formula (actually understand it lole)
- [ ] example of one > 3x3, using that easy co-factor method
/If A is trinagular the ndetA is the product of the entries on the main diagonal/
- [ ] things that change/dont change determinant

*** W10
**** More Determinants, Lay 3.2-3.3
for any nxn matrices A and B, detAB = detA * detB
- [ ] transformation shit
*Cramer's Rule*
Suppose A is invertible. Then the unique solution to Ax=b is:
$x_{i}=\frac{detA_{i}(b)}{detA}$, where $A_i(b)=A$ with coulumn /i/ replaced by /b/
- [ ] adjugate
For an nxn matrix A, the _ (area,volume,whatever comes after that) spanned by its column vectors is |detA|
If S<R^2 is a region of finite area then the area of AS=|detA| * area of S

*** W11
**** Eigenvectors and Eigenvalues, Lay 5.1-5.2
A non-zero vector is an eigenvector with eigenvalue \lambda if:
$Av=\lambda v$
- [ ] *example of 3x3 or higher*
- [ ] be super sure you know how to do null space shit
  
\lambda is an eigenvalue of A if and only if $det(A-\lambda I)=0$
 So to find eigenvalues, solve $det(A-\lambda I)=0$$det(A-\lambda I)=0$. This is a degree /n/ polynomial in \lambda
 Then for each eigenvalue \lambda_{i} we find the null-space of A-\lambda_{i}=I
 - [ ] example of Makov sequences with eigenvalues
**** Diagonalisation, Lay 5.3
- [ ] example of this, it's a tad difficult
**** Eigenvectors and Linear Transformations, Lay 5.4 (/unfinished/)
- [ ] coordinates in R^n
  
*** W12
**** Eigenvectors and Linear Transformations, rest of Lay 5.4
**** Applications to Differential Equations, Lay 5.7 (/unfinished/)
*Hessian matrix*
The Hessian matrix of f(x,y) is 
\[
H_{f} = 
\begin{bmatrix}
\frac{\delta^{2} f}{\delta x^{2}} \frac{\delta^{2} f}{\delta x \delta y} \\
\frac{\delta^{2} f}{\delta y \delta x} \frac{\delta^{2} f}{\delta y^{2}}
\end{bmatrix}
\]
and similarly with more variables
If $f$, it's first and second order partial derivatives are all continuous then $H_{f}$ is symmetric, i.e. $H_{f}=H_{f}^{T}$
Any symmetric matrix can be diagonalised, and its eigenvalues are always real
- [ ] second derivative test
- [ ] example
**** Complex Eigenvalues, Lay 5.5 (/unfinished/)
*** W13
**** More Complex Eigenvalues, Factoring as A=PCP^-1, rest of Lay 5.5
**** Discrete and Continuous Dynamical System, Lay 5.6-5.7
* TODOstuff 
** TODO Include examples noted in ^
** TODO Be sure of how to compute the various spaces and their bases of a matrix
